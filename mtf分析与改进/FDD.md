# 文件与文件目录的查找优化

# 1 背景

## 1.1 现有的FDD_TREE

迪备在文件查找中，使用了一个FDD_TREE的数据结构，用于存储文件目录树。FDD_TREE的定义如下：

```c
typedef struct TFDD_Entry {
    struct TFDD_Entry *parent;
    struct TFDD_Entry *next; /*!< next sibling entry */
    struct TFDD_Entry *first_child;
    struct TFDD_Entry *last_child;
    char *name;
    size_t name_len;
    ACE_UINT64 offset; /*!< current entry offset */
} TFDD_Entry;
```

FDD_TREE的根节点是一个空的TFDD_Entry，根节点的first_child指向第一个文件目录，根节点的last_child指向最后一个文件目录。每个文件目录节点的first_child指向第一个文件节点，last_child指向最后一个文件节点。每个文件节点的next指向下一个文件节点。

对比TFDD_Tree 处理目录的方式与 Ext2 文件系统有一些相似之处，：


|相似点||
|---|---|
|树结构|两者都使用树结构来表示目录和文件的层次关系。TFDD_Tree 使用多叉树，而 Ext2 使用的是多级索引结构。|
|节点表示|TFDD_Entry 结构体类似于 Ext2 文件系统中的 inode，包含了文件或目录的元数据（如名称、偏移量等）。|
|链表结构|TFDD_Tree 中的兄弟节点通过链表连接，类似于 Ext2 中目录项的链表结构。|



|不同点|TFDD_Tree|Ext2 文件系统|
|---|---|---|
|数据存储|主要存储元数据和指向子节点的指针|inode 直接存储文件数据的指针|
|平衡性|TFDD_Tree 没有自动平衡机制，可能导致树结构不平衡| Ext2 文件系统通过多级索引结构保持一定的平衡性。|
|访问|每个节点长度不固定，磁盘不友好，不适合持久化存储| 使用块进行管理，对磁盘存储进行优化|
|数据信息|不支持,只有文件或目录检索、权限、硬链接、用户等信息不支持| 支持全面的文件相关信息|
|实现|主要关注目录和文件的层次关系|Ext2 文件系统有更复杂的实现，包括块管理、文件权限、硬链接等，而 TFDD_Tree |

## 1.1.1 并发问题

参考fdd_tree_bulid函数，冲bulid_tree方法，是线性解析目录树，但是在解析目录树的过程中，对于的一个大型的catalog文件，构建时间会比较长。

## 1.1.2 存储优化

一般不管使用什么数据结构都是将索引放到内存中，以加快查找速度。但一般的数据库InnoDB、sqlite使用B+树作为索引的时候，内存中使用了page的管理，FDD使用大内存池的方法，这样会导致内存的浪费。另外，FDD使用的是链表结构，这样会导致内存的碎片化。

|内存管理|FDD_TREE的大内存池|数据库Page管理|
|---|---|---|
|优点|实现简单：大内存池管理的实现相对简单，只需要维护一个大的内存块和一个指针，指示当前的分配位置。分配速度快：大内存池管理的内存分配速度较快，因为只需要移动指针，不需要维护复杂的结构。|内存利用率高：Page 管理可以更好地利用内存，因为每个 Page 的大小是固定的，减少了内存碎片。缓存友好：Page 管理可以更好地利用 CPU 缓存，因为 Page 的大小通常与缓存行对齐，减少了缓存未命中率。易于分页和交换：在需要将部分数据交换到磁盘时，Page 管理更容易实现，因为每个 Page 是独立的，可以单独交换。灵活性：Page 管理可以根据需要动态分配和释放内存，适应不同的内存需求。|
|缺点|内存碎片：大内存池管理容易产生内存碎片，因为内存块的大小不固定，释放内存时可能会留下无法利用的小块。内存利用率低：由于内存碎片和固定增长策略，大内存池管理的内存利用率较低，可能会浪费大量内存。不易分页和交换：大内存池管理不容易将部分数据交换到磁盘，因为内存块是连续的，无法单独交换。不适合持久存储，磁盘IO效率低。| 管理开销：Page 管理需要维护 Page 表和空闲列表，增加了管理开销。复杂性：实现 Page 管理需要更多的代码和逻辑，增加了系统的复杂性。|

## 1.1.3 查询效率

* 单个目录或文件-点

TFDD_Tree的查询效率好情况O(1)和最坏情况O(n*d)相差较大。

Ext2文件系统的查询效率是目录O(logN)、文件O(N)的时间复杂度。

* 一个目录下的所有文件-范围

TFDD_Tree的查询效率由于包含了next指针，所以查询效率是O(n)，查询一个文件即可通过链表遍历，但不能并发，是个串行过程，NAS的一个目录下的海量文件仍可能存在瓶颈。

在 Ext2 文件系统中，目录项是以线性列表的形式存储的，每个目录项包含文件名和指向文件 inode 的指针。遍历目录时，需要逐个读取目录项。

* 通过通配符查询文件 - 随机、范围

在 FDD_Tree 中，目录和文件以树形结构存储，每个节点包含子节点的链表。匹配文件查询时，需要遍历子节点链表并进行匹配。由于子节点是以链表形式存储的，并行查询效率也不高。

对Ext2影响不大，可以并行查询。


## 1.1.4 适用场景

* FDD_TREE 适用于目录树结构简单，目录层次不深的场景，如小型文件系统等，同时一次创建、删除、查找文件的频率不高的场景。


## 1.2 ext2以及发展


虽然我们和Ext2对比了FDD的性能，但是Ext2也有自己的问题。目前，Ext2 文件系统是 Linux 系统中最传统、常用的文件系统之一，它是一个开源的文件系统，支持大多数 Linux 发行版。Ext2文件系统的设计目标之一是提供一个高性能、可靠的文件系统，同时保持简单和易于维护。

这种传统的Ext2采用顺序记录的方式组织目录文件，每个记录称作一个目录项，目录项包含文件的文件名和索引节点。由于文件名长度不固定，目录项采用变长记录以节省存储空间，另外目录项的位置并不根据某种方法排序，这导致在目录文件中搜索特定的目录项时，必须遍历整个目录的所有目录项进行对比。因此，当使用Ext2文件系统时，在大型目录中创建、删除和查找文件会非常慢。

Phillips在2001年提出一种类似于B+树的H树，用于解决Ext2在处理大目录时的瓶颈。该结构完全兼容ext2的目录索引结构，针对每个目录建立一棵Ｈ树，使用文件名的Hash值对存储目录项的目录块进行索引，而不是每个目录项都建立索引。在Linux2.6之后版本的内核中，Ext3模块都采用了这一改进，但为了保证与Ext2向前兼容，目录文件的块仍采用直接索引与间接索引相结合的方式，当目录文件增大时，多级索引仍然会带来一定的延时。

B树及其变种因具有O（lgN）的查找复杂度和在降低磁盘IO方面的出色性能，被许多现代文件系统用作数据组织的基础结构。Reiserfs文件系统使用单棵B+树组织文件系统的所有数据，包括目录，目录项由文件名Hash得到的32位关键字进行排序；文件系统元数据与文件数据由同一棵B+树进行索引，这不仅增加了结构维护的复杂度，而且文件数据增多时造成B+树的高度增长也会影响文件名的查找效率。Btrfs文件系统使用一种B+树的变种对目录项进行索引，目录项名称散列64位的关键字在B+树中排序。与Reiserfs一样，它的叶子节点之间没有连接指针，遍历相邻的叶子节点也必须逐个经历从根节点到叶子节点的分支索引过程。


# 2 B+树或跳表的选择

## 2.1 B+树

B+树是一种自平衡的树数据结构，它是一种多路搜索树，其中每个节点可以有多个子节点，但只有叶子节点才包含数据。B+树的主要特点如下：

* 所有叶子节点都在同一层，并且通过链表连接，这样可以方便地进行范围查询。
* 每个节点都有一个指向子节点的指针，这样可以方便地进行查找和遍历。
* 每个节点都有一个指向父节点的指针，这样可以方便地进行插入和删除操作。

以下是SQLite、Btrfs、Memcached和InnoDB在使用B+树进行数据检索时的设计和使用上的区别，以及它们各自的特点和性能差异的比较：

| 特性         | SQLite                  | Btrfs                        | InnoDB                                | Oracle                                |
|--------------|-------------------------|------------------------------|---------------------------------------|---------------------------------------|
| 用途         | 关系型数据库            | 文件系统                     | 关系型数据库                          | 关系型数据库                          |
| B+树用途     | 索引和数据存储          | 元数据和数据块索引           | 索引和数据存储                        | 索引和数据存储                        |
| 节点大小     | 可配置，通常为页面大小（默认4KB） | 可配置，通常为4KB或更大       | 可配置，通常为16KB                    | 可配置，通常为8KB或更大               |
| 叶子节点     | 存储实际数据            | 存储指向数据块的指针         | 存储实际数据                          | 存储实际数据                          |
| 非叶子节点   | 存储键和指向子节点的指针 | 存储键和指向子节点的指针     | 存储键和指向子节点的指针              | 存储键和指向子节点的指针              |
| 并发控制     | 使用锁和事务            | 使用CoW（写时复制）和事务    | 使用锁和MVCC（多版本并发控制）        | 使用锁和MVCC（多版本并发控制）        |
| 事务支持     | 支持ACID事务            | 支持ACID事务                 | 支持ACID事务                          | 支持ACID事务                          |
| 崩溃恢复     | 使用WAL（写前日志）     | 使用日志和快照               | 使用重做日志和撤销日志                | 使用重做日志和撤销日志                |
| 性能优化     | 支持索引优化和查询优化  | 支持延迟分配和压缩           | 支持索引优化、查询优化和缓冲池管理    | 支持索引优化、查询优化和缓冲池管理    |
| 适用场景     | 小型到中型应用程序的数据存储 | 大规模文件系统和存储         | 大型企业级应用程序的数据存储          | 大型企业级应用程序的数据存储          |



**性能差异**
* SQLite：适用于嵌入式系统和小型应用程序，性能较好，但在高并发场景下可能会有瓶颈。
* Btrfs：适用于大规模文件系统，支持快照和压缩，性能较好，但在极端高并发和大规模数据操作下可能会有性能开销。
* Memcached：主要用于高速缓存，性能极高，但不支持持久化和事务。
* InnoDB：适用于大型企业级应用程序，支持复杂查询和高并发，性能优越，但需要更多的内存和存储资源。

## 2.2 使用跳表的存储系统-Ceph

Ceph 并没有使用B+树进行数据的管理：
* OSDMap：用于管理和检索 OSD 信息，主要用于数据对象的分布和定位。使用跳表数据结构，依赖 CRUSH 算法实现数据的高可用性和负载均衡。
* MDS：用于管理和检索文件系统的元数据，主要用于文件系统的目录和文件操作。使用内存中的树结构，通过动态子树分区技术实现负载均衡和高可用性。

### 2.2.1 OSDMap

Ceph 的元数据管理器 OSDMap 使用跳表（Skip List）来存储和查找对象的位置信息。跳表是一种概率性数据结构，它通过在链表上增加多级索引来提高查找效率。跳表的时间复杂度为 O(log n)，与 B+ 树相当，但实现更简单。

跳表的主要特点如下：

* 跳表是一种概率性数据结构，它通过在链表上增加多级索引来提高查找效率。
* 跳表的时间复杂度为 O(log n)，与 B+ 树相当，但实现更简单。
* 跳表的空间复杂度为 O(n)，与 B+ 树相当。
* 跳表支持动态插入和删除操作，与 B+ 树相当。
* 跳表支持范围查询，与 B+ 树相当。
* 跳表支持并发访问，与 B+ 树相当。


跳表虽然简单且高效，但在实际应用中，B+ 树仍然更常用，主要原因如下：

* 磁盘访问效率：优化了磁盘 I/O 操作。
* 范围查询效率：叶子节点链表提高了范围查询效率。
* 成熟度和广泛应用：在数据库和文件系统中应用广泛且优化成熟。
* 内存使用：索引结构更紧凑，内存使用效率更高。
* 并发控制：天然支持并发，在高并发场景下性能和稳定性更有保障。

跳表一个明显的问题就是读放大问题，即查找一个元素时，需要访问的节点数可能远多于实际存储的元素数。在Ceph中，OSDMap的跳表结构主要用于存储和查找对象的位置信息，而不是存储实际的数据。因此，跳表的读放大问题在Ceph中并不明显，不会对性能产生太大的影响。

同时，Ceph通过其他方法优化读放大了这个问题：
* 优化跳表结构：通过调整跳表的层数和节点数量，减少不必要的读操作。
* 缓存机制：利用缓存来存储常用的数据，减少对底层存储的访问次数。
* 数据分区：通过将数据分区存储在不同的节点上，减少单个节点的负载，从而提高整体系统的读写性能。
* 并行处理：利用并行处理技术，提高数据读取的效率。

### 2.2.2 In-Memory Directory Tree

In-Memory Directory Tree 是 Ceph MDS 用来存储和查询文件系统元数据的数据结构。它是一种内存中的树形结构，用于表示文件系统的目录和文件。In-Memory Directory Tree 的主要特点如下：

* 内存中的树结构：是一种内存中的树形结构，用于表示文件系统的目录和文件。每个目录和文件都是树中的一个节点，节点之间的关系表示目录结构。
* 元数据存储：元数据（如文件名、权限、时间戳等）存储在树的节点中。MDS 将这些元数据存储在内存中，以便快速访问和修改。
* 分布式管理：在大型集群中，MDS 可以分布在多个节点上。Ceph 使用一种称为 动态子树分区（Dynamic Subtree Partitioning）的技术，将目录树动态分割并分配给不同的 MDS 节点，以实现负载均衡和高可用性。
* 一致性：使用一种称为 Raft 的分布式一致性算法来保证元数据的一致性。Raft 算法通过选举一个领导者节点来协调多个节点之间的操作，从而保证元数据的一致性。
* 缓存：使用缓存来提高查询效率。MDS 将常用的元数据存储在缓存中，以便快速访问。当缓存中的数据过期时，MDS 会从持久化存储中加载最新的元数据。
* 持久化：的元数据会定期持久化到磁盘上，以便在 MDS 重启或故障时能够恢复元数据。持久化操作通常使用日志或快照的方式。
* 查询优化：使用多种查询优化技术，如索引、预取和缓存，以提高查询效率。例如，MDS 可以使用 B+ 树索引来加速文件名的查找，或者使用预取技术来提前加载常用的元数据。（**用户自行优化实现**）
* 安全性：支持多种安全机制，如访问控制、加密和认证，以确保元数据的安全性。
* 扩展性：支持水平扩展，可以通过增加 MDS 节点来提高元数据的处理能力。
* 高可用性：支持高可用性，可以通过复制和故障转移技术来保证元数据的可用性。
* 兼容性：与 POSIX 文件系统兼容，可以与现有的文件系统工具和应用程序无缝集成。
* 性能：性能优于传统的文件系统元数据管理器，因为它使用内存中的树结构来存储和查询元数据，并且支持多种查询优化技术。


![MDS 树](mds.png)

## 2.3 使用跳表的数据库系统-LevelDB、RocketDB

LevelDB 和 RocketDB 都没有使用 B+ 树进行数据的管理，而是使用了跳表（Skip List）和 LSM 树（Log-Structured Merge Tree）。

为了解决读放大问题，使用了缓存和过滤器来优化读性能。同时，通过批量写入和压缩来优化写性能。

参照：过滤器 https://github.com/darkking1112/filters/blob/main/beony_Bloom.md

# 3 优化设计与实现

比较Btrfs的设计，并结合备份恢复、事务支持、性能优化、适用场景等方面，一个存储系统的元数据管理。

这里可以发现，一般内存数据库使用跳表（随机IO较多），而磁盘数据库（需要持久化）一般使用B+树。但也没有明显的界限，比如，rockeDB的持久化也使用了跳表，MemSQL内存数据库使用了B+树。

至于文件系统，多看元数据管理选择的数据库，例如，Ceph（相关百度CFS）选择了rocketDB，而XFS、Btrfs高性能文件系统选择了B+树。

我们希望设计系统具有以下特点：
* 数据的多池管理：支持多个数据池，每个数据池可以独立管理元数据，面向分布式进行设计，分离式的元数据管理。
* 写性能：写是备份恢复的核心，需要支持高并发写，保证写性能。
* 读性能：伴随非结构化数据的重要性提升，增量备份、合成备份、数据同步等场景对读性能要求较高，同时，未来通过分析备份的数据，提供自主的备份策略等场景，也需要支持高并发读。
* 高效的缓存管理：支持LRU、LFU等缓存淘汰策略，提高元数据访问速度。
* 高效的内存管理：支持内存分页，提高内存利用率。
* 元数据管理：将数据管理服务和元数据进行分离，提高数据管理服务的性能。
* 快照支持：支持快照功能，方便数据恢复和备份。
* 针对Nvme SSD的优化：针对Nvme SSD的特性，进行读写优化，提高存储性能。
* 小文件支持：支持小文件存储，提高存储系统的灵活性。
* 并发控制：支持并发访问，通过降低锁的粒度，并根据具体操作语义（如读、写、修改）进一步细化锁的范围，来提高存储系统的并发性能。再小的锁也是不可避免的，当用户用户增加到一定的程度，IO瓶颈将很快出现，那么数据的放置策略就非常重要了。
* 数据回收机制：支持数据回收机制，提高存储系统的空间利用率。
* 分布式场景下一致性问题：基于某个开源项目实现Raft协议，保证分布式场景下的一致性和高可用的问题。


## 3.1 B+树的设计

